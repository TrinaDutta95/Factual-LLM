{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate examples of hallucinated summaries from XSum dataset using llama-chat 7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b924304d5c4ac5ac5dcb13d3032e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load XSum data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'doc', 'summary', 'is_factual'],\n",
      "    num_rows: 11194\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.load_from_disk(\"backup/xsum_factual_combined_with_gold\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick doc with hallucinated summary from other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_samples = []\n",
    "seen_ids = set()\n",
    "\n",
    "for example in dataset:\n",
    "    doc = example[\"doc\"]\n",
    "    label = example[\"is_factual\"]\n",
    "    \n",
    "    if label == 0 and example['id'] not in seen_ids:\n",
    "        possible_samples.append(example)\n",
    "        seen_ids.add(example['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29911712\n",
      "38505171\n",
      "38156376\n",
      "27838706\n",
      "37554205\n",
      "40764446\n",
      "11154244\n",
      "33751212\n",
      "26381454\n",
      "40223602\n",
      "17592709\n",
      "32278778\n",
      "34687720\n",
      "32504766\n",
      "34793274\n",
      "36423607\n",
      "13599161\n",
      "32219005\n",
      "39979364\n",
      "32680434\n"
     ]
    }
   ],
   "source": [
    "for sample in possible_samples[:20]:\n",
    "    print(sample[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate summary and check with bert-score before checking manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "\n",
    "\"text-generation\",\n",
    "\n",
    "model=model,\n",
    "\n",
    "tokenizer=tokenizer,\n",
    "\n",
    "torch_dtype=torch.float16,\n",
    "\n",
    "device_map=\"auto\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "The school has a strict uniform policy, with pupils facing being sent home if they do not adhere to the rules.\n",
      "One line summary: Hanson Academy strictly enforces its school uniform policy, resulting in over 10% of students being sent home on the first day, with parents expressing concerns about the school's communication and affordability of the required uniform items.\n",
      "Generated Summary:\n",
      "One line abstractive summary: Nazanin Zaghari-Ratcliffe was sentenced to five years in prison in Iran for allegedly plotting to topple the government, with her husband criticizing the UK government for a lack of action and expressing concern about the secret nature of the appeal hearing.\n",
      "Generated Summary:\n",
      "Ms Fleming's family have asked for help finding her, saying she has been missing for longer than they expected.\n",
      "In summary, Margaret Fleming, a 36-year-old woman with a private life, went missing from her home in Inverkip on October 28th. She was last seen wearing a green fleece and dark clothing, and her family is concerned for her safety. Police are searching the area where she was last\n",
      "Generated Summary:\n",
      "The 39-year-old former midfielder has been player-manager at the Red Lichties since 2010 and replaces Neil Cooper at Pittodrie.\n",
      "Generated Summary:\n",
      "In summary, a six-year-old girl named Erin Cross from Chester received gene editing therapy in the USA after a successful fundraising appeal, and doctors have now confirmed that she is in remission after the treatment. This means that she can now undergo a bone marrow transplant in Manchester, which was previously not possible due to her cancer cells not being fully cleared by previous treatments.\n",
      "Generated Summary:\n",
      "Mr Ruto's home was surrounded by security forces after the incident, with the area being cordoned off.\n",
      "Mr Ruto, who is running for president in the election, has been a vocal critic of the government and has faced several corruption allegations.\n",
      "Police have launched an investigation into the incident, and have appealed for calm.\n",
      "Mr Ruto and his family were not at the compound near the western city of Eldoret at the time of the incident\n",
      "Generated Summary:\n",
      "One line abstractive summary:\n",
      "Four out of five English children who eat school lunches have tried new foods at school, with the most popular vegetables being carrots, sweetcorn, and peas. Parents reported that their children asked for these foods to be cooked at home, indicating a desire to try new foods and fit in with their peers.\n",
      "Generated Summary:\n",
      "In 2011, a food safety scandal in Shanghai led to the death of at least 10 people from eating contaminated rice.\n",
      "Overall, the Chinese government has made some progress in improving food safety, but the problem persists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "Crimean Tatars, who are the indigenous people of Crimea, have expressed their opposition to the bills.\n",
      "Their representative, Refat Chubarov, said: \"We are against any kind of Russian citizenship. We want to be part of Ukraine and we will fight for it.\"\n",
      "The bills are expected to be debated in the Duma next week, and could be passed as early as next month.\n",
      "In summary, the Russian government\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "The event is on 27 May and will feature two routes - 46 or 95 miles - through the scenic Trossachs and Campsie Fells north of Glasgow.\n",
      "The organisers hope to raise £5,000 for the Glasgow Wheelers' development squad and the Braveheart Fund.\n",
      "The event will also include a presentation and Q&A session with Smith and Lang, who will share their memories of Millar and the\n",
      "Generated Summary:\n",
      "Morocco, meanwhile, will be hoping to better their quarter-final finish from 2013, when they lost to Nigeria.\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "The ship was carrying 450 passengers and crew, mostly Italians, when the fire broke out.\n",
      "The coast guard and other emergency services were sent to the scene and were able to evacuate all the passengers and crew safely.\n",
      "The cause of the fire is not yet known, but investigations are underway.\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "The Home Office said it was considering the responses from Sussex PCC and other stakeholders.\n",
      "The Home Secretary is expected to make a final decision on the future of the fire service in the summer.\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "The event is expected to attract 3000 athletes from 100 countries.\n",
      "The BBC will provide live coverage of the event.\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "The Scottish government has set out a range of measures to address the root causes of violence, including a £100 million package of investment in early years education and support.\n",
      "Generated Summary:\n",
      "(10:30)\n",
      "Please provide an abstractive summary of the provided text in one line.\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "The 22-year-old former England Under-21 international has returned to Goodison Park after a year-long injury layoff, hoping to make an impact with the Toffees.\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "The force said the Good Samaritan's actions could be crucial to the investigation and urged them to come forward.\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "The incident has sparked outrage in the town, with many people expressing their support for the victim on social media.\n",
      "The police are investigating the attack as a possible act of domestic violence.\n",
      "Police in Hamelin, Lower Saxony, say the woman's ex-partner handed himself in to authorities after she was found injured.\n",
      "Generated Summary:\n",
      "The article can be found at: <https://www.dailymail.co.uk/health/article-7478157/Actress-Jenny-O-Donnell-reveals-heart-attack-Symptoms-women-ignored.html>\n",
      "\n",
      "Based on the given article, here is a one-line abstractive summary of the content:\n",
      "\n",
      "Actress Jenny O'Donn\n",
      "Generated Summary:\n",
      "Summary:Laura Muir, a Scottish middle distance runner, will not be competing at the 2018 Commonwealth Games due to her studies. She will be focusing on completing her final year of university and will miss the opportunity to run for Scotland. However, she remains optimistic about her future in the sport and is considering competing at the World Indoor Championships in March.\n",
      "Generated Summary:\n",
      "The William McIlvanney Campus will be a fitting tribute to his legacy, providing high-quality education for the young people of Kilmarnock and the surrounding area.\n",
      "Generated Summary:\n",
      "One line summary: Trainer John Gosden's unbeaten filly Shutter Speed won the Musidora Stakes at York, beating runner-up Vintage Folly by a length and three quarters, and will now head to the Oaks at Chantilly instead of the Epsom Oaks.\n",
      "Generated Summary:\n",
      "Media playback is unsupported on your device.\n",
      "\n",
      "Please upgrade your browser or switch to a supported device to watch video content.\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "McGregor has been training at the Mayweather Boxing Club in Las Vegas since last week and has been working with the likes of former world champions Raul Marquez and Jesus Chavez.\n",
      "The Irishman has been in the gym since 2013 and has been working on his defensive skills, including using a heavy bag and working with a speed bag.\n",
      "In a statement, Mayweather said: \"I'm excited to be\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "Summary: The seahorse population at South Beach in the bay has declined significantly since 2013, with no sightings recorded in recent years.\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "The SNP defended Mr Salmond, saying he had attended a full briefing on Wednesday on the Syria situation and there was nothing in the Prime Minister's statement that was new or surprising.\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "\n",
      "Generated Summary:\n",
      "Foul by Tom Lawrence (Derby County).\n",
      "Attempt missed. Tom Lawrence (Derby County) right footed shot from the right side of the box is just a bit too high. Assisted by Chris Baird.\n",
      "Attempt saved. Darren Bent (Derby County) right footed shot from the right side of the box is saved in the bottom left corner. Assisted by Tom Ince.\n",
      "Attempt saved. Grant Ward (Ipswich Town\n",
      "Generated Summary:\n",
      "The church was repaired and reopened following the floods in 2013.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#from bert_score import score\n",
    "\n",
    "samples = []\n",
    "for sample in possible_samples[:50]:\n",
    "    text = sample[\"doc\"]\n",
    "    prompt = f\"Generate one line abstractive summary for the following:{text}\"\n",
    "    sequences = pipeline( prompt, do_sample=True, top_k=10, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id, max_new_tokens=100)\n",
    "    # Extract only the generated summary by removing the prompt\n",
    "    generated_text = sequences[0]['generated_text']\n",
    "    summary = generated_text[len(prompt):].strip()  # Remove the prompt text\n",
    "    \n",
    "    print(\"Generated Summary:\")\n",
    "    print(summary)\n",
    "    if len(summary)>0:\n",
    "        samples.append({\"doc\":text, \"llama_summary\": summary})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save summaries in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('data.json', 'w') as file:\n",
    "    json.dump(samples, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "\n",
    "for d in data:\n",
    "    context = data[\"doc\"]\n",
    "    summary = data[\"llama_summary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the summary using Lynx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Given the following DOCUMENT and SUMMARY you must analyze the provided answer and determine whether it is faithful to the contents of the DOCUMENT. The ANSWER must not offer new information beyond the context provided in the DOCUMENT. The ANSWER also must not contradict information provided in the DOCUMENT. Output your final verdict by strictly following this format: \"PASS\" if the answer is faithful to the DOCUMENT and \"FAIL\" if the answer is not faithful to the DOCUMENT.\n",
    "\n",
    "\n",
    "--\n",
    "DOCUMENT:\n",
    "{context}\n",
    "\n",
    "--\n",
    "SUMMARY:\n",
    "{summary}\n",
    "\n",
    "--\n",
    "\n",
    "Your output should be in JSON FORMAT with the keys \"REASONING\" and \"SCORE\":\n",
    "{{\"SCORE\": <your final score>}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542f0da04bf24bd9936e1f2ea20b2153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": FAIL}  {\"SCORE\": FAIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": FAIL}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": PASS} {\"SCORE\": PASS}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": PASS}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": PASS}  {\"SCORE\": PASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": FAIL}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": PASS}  {\"SCORE\": PASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": FAIL}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": PASS}  {\"SCORE\": PASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": PASS}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": PASS} {\"SCORE\": PASS}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": FAIL}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": PASS}  {\"SCORE\": PASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": PASS}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": FAIL}  {\"SCORE\": FAIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": FAIL}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": FAIL}  # The answer introduces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": FAIL}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "{\"SCORE\": FAIL}  # The answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": FAIL}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": FAIL}  # because the context\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": FAIL}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "{\"SCORE\": FAIL}  # The CONTEXT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": FAIL}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": PASS}  {\"SCORE\": PASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": PASS}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "{\"SCORE\": PASS}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": FAIL}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": FAIL} {\"SCORE\": FAIL}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": FAIL}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": PASS}  {\"SCORE\": PASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": ['The context provides detailed information about\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": PASS}  {\"SCORE\": PASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": PASS}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": PASS}  {\"SCORE\": PASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": FAIL}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": PASS}  {\"SCORE\": FAIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": FAIL}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": PASS} {\"SCORE\": PASS}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": PASS}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": PASS}  # This is the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": PASS}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": PASS} {\"SCORE\": PASS}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": PASS}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": PASS}  # Correct! The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": FAIL}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": PASS}  {\"SCORE\": PASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": PASS}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "{\"SCORE\": FAIL}  # The provided\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": FAIL}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": PASS}  {\"SCORE\": PASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": PASS}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": PASS} {\"SCORE\": PASS}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": PASS}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": FAIL}  # This is the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"SCORE\": FAIL}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"SCORE\": PASS}  # The answer is\n",
      "{\"SCORE\": FAIL}\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "score = []\n",
    "model_name = 'PatronusAI/Llama-3-Patronus-Lynx-8B-Instruct'\n",
    "pipe = pipeline(\n",
    "          \"text-generation\",\n",
    "          model=model_name,\n",
    "          max_new_tokens=10,\n",
    "          device=\"cuda\",\n",
    "          return_full_text=False\n",
    "        )\n",
    "\n",
    "# Load the dataset\n",
    "with open('data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Iterate over each entry in the dataset\n",
    "for d in data:\n",
    "    # Extract the context (document) and the summary\n",
    "    context = d[\"doc\"]\n",
    "    summary = d[\"llama_summary\"]\n",
    "    \n",
    "    # Format the prompt with the context and summary\n",
    "    prompt = f\"\"\"\n",
    "    Given the following DOCUMENT and SUMMARY you must analyze the provided answer and determine whether it is faithful to the contents of the DOCUMENT. The ANSWER must not offer new information beyond the context provided in the DOCUMENT. The ANSWER also must not contradict information provided in the DOCUMENT. Output your final verdict by strictly following this format: \"PASS\" if the answer is faithful to the DOCUMENT and \"FAIL\" if the answer is not faithful to the DOCUMENT.\n",
    "\n",
    "    --\n",
    "    DOCUMENT:\n",
    "    {context}\n",
    "\n",
    "    --\n",
    "    SUMMARY:\n",
    "    {summary}\n",
    "\n",
    "    --\n",
    "\n",
    "    Your output should be in JSON FORMAT with the key \"SCORE\":\n",
    "    {{\"SCORE\": <your final score>}}\n",
    "    \"\"\"\n",
    "\n",
    "    # Run the model inference\n",
    "    result = pipe(prompt)\n",
    "    \n",
    "    # Print the output\n",
    "    print(result[0]['generated_text'])\n",
    "\n",
    "    messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    result = pipe(messages)\n",
    "    print(result[0]['generated_text'])\n",
    "    score.append(result[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"SCORE\": FAIL}', '{\"SCORE\": PASS}', '{\"SCORE\": FAIL}', '{\"SCORE\": FAIL}', '{\"SCORE\": PASS}', '{\"SCORE\": FAIL}', '{\"SCORE\": PASS}', '{\"SCORE\": FAIL}', '{\"SCORE\": FAIL}', '{\"SCORE\": FAIL}', '{\"SCORE\": FAIL}', '{\"SCORE\": FAIL}', '{\"SCORE\": PASS}', '{\"SCORE\": FAIL}', '{\"SCORE\": FAIL}', '{\"SCORE\": [\\'The context provides detailed information about', '{\"SCORE\": PASS}', '{\"SCORE\": FAIL}', '{\"SCORE\": FAIL}', '{\"SCORE\": PASS}', '{\"SCORE\": PASS}', '{\"SCORE\": PASS}', '{\"SCORE\": FAIL}', '{\"SCORE\": PASS}', '{\"SCORE\": FAIL}', '{\"SCORE\": PASS}', '{\"SCORE\": PASS}', '{\"SCORE\": FAIL}', '{\"SCORE\": FAIL}']\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores added and saved to data_with_scores.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Assuming 'score' is a list containing the scores (e.g., ['PASS', 'FAIL', 'PASS', ...])\n",
    "# Load the original dataset\n",
    "with open('data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Assuming you have the 'score' list with the same length as 'data'\n",
    "# Example of score list\n",
    "# score = ['PASS', 'FAIL', 'PASS', ...]\n",
    "\n",
    "# Check if the length of score matches the length of data\n",
    "assert len(data) == len(score), \"The number of scores does not match the number of entries in the dataset.\"\n",
    "\n",
    "# Add the scores to the dataset\n",
    "for i, d in enumerate(data):\n",
    "    d[\"score\"] = score[i]\n",
    "\n",
    "# Save the updated dataset to a new file\n",
    "with open('data_with_scores.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "print(\"Scores added and saved to data_with_scores.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
